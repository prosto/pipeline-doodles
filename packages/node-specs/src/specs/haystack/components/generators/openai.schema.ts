import type { NodeJsonSchema } from "@/types";
import { schemaId } from "@repo/node-specs/schema";

export const schema: NodeJsonSchema = {
  $schema: "https://json-schema.org/draft/2020-12/schema",
  $id: schemaId("/haystack/components/generators/openai"),
  title: "OpenAIGenerator",
  description:
    "Generates text using OpenAI's large language models (LLMs).\n\nIt works with the gpt-4 and gpt-3.5-turbo models and supports streaming responses\nfrom OpenAI API. It uses strings as input and output.\n\nYou can customize how the text is generated by passing parameters to the\nOpenAI API. Use the `**generation_kwargs` argument when you initialize\nthe component or when you run it. Any parameter that works with\n`openai.ChatCompletion.create` will work here too.\n\n\nFor details on OpenAI API parameters, see\n[OpenAI documentation](https://platform.openai.com/docs/api-reference/chat).\n\n### Usage example\n\n```python\nfrom haystack.components.generators import OpenAIGenerator\nclient = OpenAIGenerator()\nresponse = client.run(\"What's Natural Language Processing? Be brief.\")\nprint(response)\n\n>> {'replies': ['Natural Language Processing (NLP) is a branch of artificial intelligence that focuses on\n>> the interaction between computers and human language. It involves enabling computers to understand, interpret,\n>> and respond to natural human language in a way that is both meaningful and useful.'], 'meta': [{'model':\n>> 'gpt-4o-mini', 'index': 0, 'finish_reason': 'stop', 'usage': {'prompt_tokens': 16,\n>> 'completion_tokens': 49, 'total_tokens': 65}}]}\n```",
  type: "object",
  __pyType: "OpenAIGenerator",
  __pyModule: "haystack.components.generators.openai",
  __nodeType: "component",
  __defaultName: "generator",

  $defs: {
    initParameters: {
      type: "object",
      properties: {
        api_key: {
          $ref: "/haystack/utils/auth/secret",
          description: "The OpenAI API key to connect to OpenAI.",
          default: {
            type: "env_var",
            env_vars: ["OPENAI_API_KEY"],
            strict: true,
          },
        },
        model: {
          type: "string",
          default: "gpt-4o-mini",
          __pyType: "str",
          description: "The name of the model to use.",
        },
        streaming_callback: {
          type: "string",
          default: null,
          __pyType:
            "typing.Optional[typing.Callable[[haystack.dataclasses.streaming_chunk.StreamingChunk], NoneType]]",
          description:
            "A callback function that is called when a new token is received from the stream.\nThe callback function accepts StreamingChunk as an argument.",
        },
        api_base_url: {
          type: "string",
          default: null,
          __pyType: "typing.Optional[str]",
          description: "An optional base URL.",
        },
        organization: {
          type: "string",
          default: null,
          __pyType: "typing.Optional[str]",
          description: "The Organization ID, defaults to `None`.",
        },
        system_prompt: {
          type: "string",
          default: null,
          __pyType: "typing.Optional[str]",
          description:
            "The system prompt to use for text generation. If not provided, the system prompt is\nomitted, and the default system prompt of the model is used.",
        },
        generation_kwargs: {
          type: "object",
          additionalProperties: true,
          default: null,
          __pyType: "typing.Optional[typing.Dict[str, typing.Any]]",
          description:
            "Other parameters to use for the model. These parameters are all sent directly to\nthe OpenAI endpoint. See OpenAI [documentation](https://platform.openai.com/docs/api-reference/chat) for\nmore details.\nSome of the supported parameters:\n- `max_tokens`: The maximum number of tokens the output text can have.\n- `temperature`: What sampling temperature to use. Higher values mean the model will take more risks.\n    Try 0.9 for more creative applications and 0 (argmax sampling) for ones with a well-defined answer.\n- `top_p`: An alternative to sampling with temperature, called nucleus sampling, where the model\n    considers the results of the tokens with top_p probability mass. So, 0.1 means only the tokens\n    comprising the top 10% probability mass are considered.\n- `n`: How many completions to generate for each prompt. For example, if the LLM gets 3 prompts and n is 2,\n    it will generate two completions for each of the three prompts, ending up with 6 completions in total.\n- `stop`: One or more sequences after which the LLM should stop generating tokens.\n- `presence_penalty`: What penalty to apply if a token is already present at all. Bigger values mean\n    the model will be less likely to repeat the same token in the text.\n- `frequency_penalty`: What penalty to apply if a token has already been generated in the text.\n    Bigger values mean the model will be less likely to repeat the same token in the text.\n- `logit_bias`: Add a logit bias to specific tokens. The keys of the dictionary are tokens, and the\n    values are the bias to add to that token.",
        },
        timeout: {
          type: "number",
          default: null,
          __pyType: "typing.Optional[float]",
          description:
            "Timeout for OpenAI Client calls, if not set it is inferred from the `OPENAI_TIMEOUT` environment variable\nor set to 30.",
        },
        max_retries: {
          type: "integer",
          default: null,
          __pyType: "typing.Optional[int]",
          description:
            "Maximum retries to establish contact with OpenAI if it returns an internal error, if not set it is inferred\nfrom the `OPENAI_MAX_RETRIES` environment variable or set to 5.",
        },
      },
      required: ["api_key", "model"],
      additionalProperties: false,
    },
    inputTypes: {
      type: "object",
      properties: {
        prompt: {
          type: "string",
          __pyType: "str",
          description: "The string prompt to use for text generation.",
        },
        streaming_callback: {
          type: "string",
          default: null,
          __pyType:
            "typing.Optional[typing.Callable[[haystack.dataclasses.streaming_chunk.StreamingChunk], NoneType]]",
          description:
            "A callback function that is called when a new token is received from the stream.",
        },
        generation_kwargs: {
          type: "object",
          additionalProperties: true,
          default: null,
          __pyType: "typing.Optional[typing.Dict[str, typing.Any]]",
          description:
            "Additional keyword arguments for text generation. These parameters will potentially override the parameters\npassed in the `__init__` method. For more details on the parameters supported by the OpenAI API, refer to\nthe OpenAI [documentation](https://platform.openai.com/docs/api-reference/chat/create).",
        },
      },
      required: ["prompt"],
      additionalProperties: false,
    },
    outputTypes: {
      type: "object",
      properties: {
        replies: {
          type: "array",
          items: {
            type: "string",
          },
          __pyType: "typing.List[str]",
        },
        meta: {
          type: "array",
          items: {
            type: "object",
            additionalProperties: true,
          },
          __pyType: "typing.List[typing.Dict[str, typing.Any]]",
        },
      },
      required: ["replies", "meta"],
      additionalProperties: false,
    },
  },
};
